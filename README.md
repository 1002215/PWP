  Deep Space Exploration Inc. (DSE) requires a working, programmable prototype of a Mars rover with full automation. The robot must utilize computer vision and image overlaying to properly navigate a path on the rough terrain of Mars. As an intermediate solution to this problem, the Chosen Scrum has been tasked to show an image overlay on a live video stream on Earth. The video stream will consist of thirty seconds of driving straight ahead on a road, making a left turn on a 4-way intersection, another thirty seconds of driving straight ahead, a right turn on a 4-way intersection, and then a final thirty seconds of driving straight ahead. The objective of the project is to have the image overlay draw lines over the lanes on the road in red with a centerline drawn in green. The program will constantly calculate these three lines, so there must be no flickers or breaks. 
  
  Moreover, there must be a compass rose in the top right corner of the video moving in the direction that the vehicle turns in the video, with the default position being north. Some further requirements include the use of perspective transform, Hough lines, canny edge detection, Gaussian blur, converting to grayscale, the .polyfit function, and a skeletonizing algorithm. In the future, this project will expand to automation on a Mars rover prototype.
  
  The methodology used to carry out this project relies heavily on knowledge of computer vision using libraries such as OpenCV, numpy, and Matplotlib. The program will begin by taking in a live video stream and applying image processing to each frame. First, each frame shall be blurred using Gaussian blur, reducing the noise in the image and making it easier to process in further steps. Each frame shall be converted into Hue, saturation, and Value (HSV) color space using an OpenCV function, allowing for the differentiation of colors based on their brightness. Since the lines of the road are all either yellow or white in the United States, OpenCV will be harnessed to create a mask so the program only executes on spaces detected as white or yellow. This is done by defining the upper and lower HSV values of white and yellow. The frame is converted to grayscale to help simplify any algorithms applied from here on out. Canny edge detection is then utilized, returning the edges of the image. Afterward, the program will invoke a triangular region of interest to reduce noise and focus on the road lines. This mimics the perspective of a car driving down a road.
  
  This region of interest will be altered to apply perspective transform. Perspective transform straightens objects that were recorded at an angle, correcting any distortions as a result of the perspective. This will return an image of the lane lines as two straight lines, making it much easier to process and detect. Hough transform will be exercised, detecting any straight lines, even accounting for slight distortion. Using the points returned by the Hough function, the lines will be grouped and drawn in red based on their slope and intercept. The lane lines have been detected and drawn on the original frame. The Guo Hall skeletonizing algorithm will use these two lines and draw a pixel-wide centerline between them. Using polyfit., the pixels can be used to draw the complete centerline in green on the original frame. This iterates for each frame processed by the camera. For the compass rose, trigonometry will determine the direction of the vehicle. A right triangle will be drawn with the hypotenuse being the centerline. Using the angle of elevation and the tangent function, the exact degree can be calculated. The quadrant the angle is in will determine the direction. For example, if the degree measures about 10Â°, then it is apparent that the vehicle is turning right since the degree measure will be in the first quadrant close to the x-axis.
